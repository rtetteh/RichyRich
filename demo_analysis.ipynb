{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec921b1",
   "metadata": {},
   "source": [
    "# Modular Empirical Size Analysis System Demo\n",
    "\n",
    "This notebook demonstrates the usage of the newly refactored, modular Python system for Empirical Size Analysis of Hybrid LogNormal-Pareto distributions.\n",
    "\n",
    "## Overview of the New System\n",
    "\n",
    "The original monolithic notebook has been refactored into a clean, maintainable package structure under `src/`:\n",
    "\n",
    "*   **`src.distribution`**: Handles data generation for the Hybrid LogNormal-Pareto distribution.\n",
    "*   **`src.estimation`**: Contains robust parameter estimation logic (MLE, Hill, Method of Moments).\n",
    "*   **`src.statistics`**: Computes the vector of auxiliary statistics used for testing.\n",
    "*   **`src.testing`**: Implements the hypothesis testing logic (Test 1, Test 1opt) and Beta selection.\n",
    "*   **`src.simulation`**: Orchestrates the full Monte Carlo simulation using parallel processing.\n",
    "\n",
    "This modular approach allows for easier testing, maintenance, and reuse of individual components.\n",
    "\n",
    "## 1. System Initialization and Configuration\n",
    "\n",
    "First, we need to set up the environment and import our new modules. We ensure the `src` directory is in the Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54327727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the current directory to the path so we can import from src\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import our new modular components\n",
    "from src.distribution import HybridLogNormalPareto, HybridParams\n",
    "from src.estimation import ParameterEstimator\n",
    "from src.statistics import AuxiliaryStatistics\n",
    "from src.testing import HypothesisTest\n",
    "from src.simulation import SimulationRunner\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"System initialized and modules loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f259b0",
   "metadata": {},
   "source": [
    "## 2. Data Generation (Distribution Module)\n",
    "\n",
    "The `src.distribution` module encapsulates the logic for generating the hybrid data. It ensures that the threshold $\\tau$ is placed exactly at the specified percentile.\n",
    "\n",
    "We define our parameters using the `HybridParams` dataclass, which provides type safety and clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "params = HybridParams(\n",
    "    mu=9.0,\n",
    "    sigma=np.sqrt(1.17),  # Note: Input is standard deviation\n",
    "    alpha=3.05,\n",
    "    tau=32000,\n",
    "    tau_perc=0.82\n",
    ")\n",
    "\n",
    "# Generate a sample\n",
    "n_samples = 5000\n",
    "data = HybridLogNormalPareto.generate_sample(n_samples, params, seed=42)\n",
    "\n",
    "print(f\"Generated {len(data)} samples.\")\n",
    "print(f\"Min: {data.min():.2f}, Max: {data.max():.2f}\")\n",
    "\n",
    "# Verify percentile placement\n",
    "actual_percentile = (np.sum(data <= params.tau) / n_samples)\n",
    "print(f\"Target Percentile: {params.tau_perc:.4f}\")\n",
    "print(f\"Actual Percentile: {actual_percentile:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data, bins=100, log_scale=(True, False))\n",
    "plt.axvline(params.tau, color='red', linestyle='--', label=f'Tau = {params.tau}')\n",
    "plt.title(\"Generated Hybrid LogNormal-Pareto Distribution (Log Scale X)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fa490a",
   "metadata": {},
   "source": [
    "## 3. Parameter Estimation (Estimation Module)\n",
    "\n",
    "The `src.estimation` module provides tools to estimate the parameters ($\\mu, \\sigma, \\alpha$) back from the data, given a threshold $\\tau$. It uses robust methods for the Pareto tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate parameters assuming we know the true tau\n",
    "mu_hat, sigma2_hat, alpha_hat = ParameterEstimator.estimate_parameters(params.tau, data)\n",
    "\n",
    "print(\"Parameter Estimation Results:\")\n",
    "print(f\"{'Parameter':<10} {'True':<10} {'Estimated':<10} {'Error %':<10}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Mu':<10} {params.mu:<10.4f} {mu_hat:<10.4f} {abs(mu_hat-params.mu)/params.mu*100:<10.2f}\")\n",
    "print(f\"{'Sigma^2':<10} {params.sigma**2:<10.4f} {sigma2_hat:<10.4f} {abs(sigma2_hat-params.sigma**2)/params.sigma**2*100:<10.2f}\")\n",
    "print(f\"{'Alpha':<10} {params.alpha:<10.4f} {alpha_hat:<10.4f} {abs(alpha_hat-params.alpha)/params.alpha*100:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07cef84",
   "metadata": {},
   "source": [
    "## 4. Auxiliary Statistics (Statistics Module)\n",
    "\n",
    "The `src.statistics` module computes a comprehensive vector of statistics (percentiles, ratios, inequality measures) that capture the shape of the distribution. These are used as the basis for the hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd81c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for a specific moment order (e.g., 4)\n",
    "order = 4\n",
    "stats_vector = AuxiliaryStatistics.compute_vector(data, order)\n",
    "\n",
    "print(f\"Auxiliary Statistics Vector (Order {order}):\")\n",
    "print(f\"Shape: {stats_vector.shape}\")\n",
    "print(f\"First 5 values: {stats_vector[:5]}\")\n",
    "\n",
    "# The vector includes percentiles, ratios, Gini, Theil, etc.\n",
    "# We can inspect specific components if needed by using the compute method directly\n",
    "single_stat = AuxiliaryStatistics.compute(data, 1) # First statistic\n",
    "print(f\"First statistic (25th percentile): {single_stat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e53544",
   "metadata": {},
   "source": [
    "## 5. Hypothesis Testing (Testing Module)\n",
    "\n",
    "The `src.testing` module contains the core logic for:\n",
    "1.  **Evaluating Tau**: Finding the threshold that minimizes the distance between observed and simulated statistics.\n",
    "2.  **Beta Selection**: Using cross-validation to find the optimal regularization parameter for the weighted test.\n",
    "3.  **Test Statistics**: Computing the unweighted (Test 1) and weighted (Test 1opt) statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683897ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate Optimal Tau\n",
    "# We search over a grid of percentiles\n",
    "tau_grid_percentiles = np.arange(70, 90, 2)\n",
    "tau_values = np.percentile(data, tau_grid_percentiles)\n",
    "\n",
    "print(\"Evaluating optimal tau...\")\n",
    "best_tau = HypothesisTest.evaluate_tau(data, tau_values, order=4, S=50) # Reduced S for demo speed\n",
    "print(f\"Best Tau: {best_tau:.2f}\")\n",
    "print(f\"True Tau: {params.tau:.2f}\")\n",
    "\n",
    "# 2. Beta Selection (Cross-Validation)\n",
    "# This is computationally intensive, so we run it on a small scale here\n",
    "c_values = np.array([10, 1.0, 0.1])\n",
    "N = len(data)\n",
    "\n",
    "print(\"\\nSelecting optimal beta...\")\n",
    "best_beta = HypothesisTest.select_optimal_beta(\n",
    "    N=N,\n",
    "    c_values=c_values,\n",
    "    moment_order=4,\n",
    "    S=20,  # Reduced for demo\n",
    "    R=20,  # Reduced for demo\n",
    "    true_data=data,\n",
    "    best_tau=best_tau,\n",
    "    tau_grid=tau_grid_percentiles,\n",
    "    seed=42\n",
    ")\n",
    "print(f\"Selected Beta: {best_beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f54653",
   "metadata": {},
   "source": [
    "## 6. Full Simulation (Simulation Module)\n",
    "\n",
    "The `src.simulation` module puts it all together. It runs the full Monte Carlo experiment to calculate the empirical size of the tests. It uses `joblib` for parallel execution.\n",
    "\n",
    "This replaces the `application_analysisl` and `application_analysisl_parallel` functions from the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e07895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the simulation runner\n",
    "# We use small parameters for this demo to ensure it runs quickly\n",
    "runner = SimulationRunner(\n",
    "    n_data=1000,       # Smaller sample size\n",
    "    M=5,               # Only 5 Monte Carlo repetitions\n",
    "    S=50,              # Fewer simulated stats\n",
    "    R=50,              # Fewer bootstrap reps\n",
    "    orders=[4],        # Only test order 4\n",
    "    n_jobs=-1,         # Use all available cores\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Starting simulation...\")\n",
    "results_df = runner.run()\n",
    "\n",
    "print(\"\\nSimulation Results:\")\n",
    "display(results_df)\n",
    "\n",
    "# Visualize the results (if we had more orders/rows)\n",
    "if len(results_df) > 0:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    results_melted = results_df.melt(\n",
    "        id_vars=['Order'], \n",
    "        value_vars=['Test 1', 'Test 1opt'],\n",
    "        var_name='Test Type',\n",
    "        value_name='Empirical Size'\n",
    "    )\n",
    "    sns.barplot(data=results_melted, x='Order', y='Empirical Size', hue='Test Type')\n",
    "    plt.axhline(0.05, color='red', linestyle='--', label='Nominal Size (0.05)')\n",
    "    plt.title(\"Empirical Size by Test Type\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc24b3",
   "metadata": {},
   "source": [
    "## 7. Comparison with Original Implementation\n",
    "\n",
    "### Key Improvements\n",
    "\n",
    "1.  **Modularity**: The logic is no longer trapped in a single 1700-line notebook cell. Each component (Distribution, Estimation, Statistics, Testing) is in its own file.\n",
    "2.  **Parallelization**: We replaced the manual `multiprocessing` code with `joblib`, which is more robust and standard in the Python data science ecosystem.\n",
    "3.  **Type Safety**: The new code uses Python type hints (`typing`), making it easier to understand what data is being passed around.\n",
    "4.  **Maintainability**: The `HybridParams` dataclass prevents \"magic numbers\" and argument confusion.\n",
    "\n",
    "### Mapping\n",
    "\n",
    "| Original Notebook Function | New System Component |\n",
    "| :--- | :--- |\n",
    "| `generate_hybrid_sample` | `src.distribution.HybridLogNormalPareto.generate_sample` |\n",
    "| `estimate_theta_improved` | `src.estimation.ParameterEstimator.estimate_parameters` |\n",
    "| `compute_auxiliary_statistics` | `src.statistics.AuxiliaryStatistics.compute_vector` |\n",
    "| `select_optimal_beta` | `src.testing.HypothesisTest.select_optimal_beta` |\n",
    "| `application_analysisl` | `src.simulation.SimulationRunner.run` |\n",
    "| `application_analysisl_parallel` | `src.simulation.SimulationRunner.run` (via `n_jobs` param) |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
