{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def application_analysisl(n_data=2000, mu_true=9.0, s2_true=1.17, alpha_true=3.05, \n",
    "                          tau_true=32000, tau_perc_true=0.82, M=10, S=200, R=299, B=299, \n",
    "                          alpha=0.05, orders=[4,8], c_values=None, tau_grid=None, \n",
    "                          seed=1234, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform empirical size analysis for statistical tests using internally generated data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_data : int, default=2000\n",
    "        Sample size for generated data\n",
    "    mu_true : float, default=9.0\n",
    "        True mu parameter for data generation\n",
    "    s2_true : float, default=1.17\n",
    "        True variance parameter for data generation\n",
    "    alpha_true : float, default=3.05\n",
    "        True alpha parameter for data generation\n",
    "    tau_true : float, default=32000\n",
    "        True tau parameter for data generation\n",
    "    tau_perc_true : float, default=0.82\n",
    "        True proportion below tau for data generation\n",
    "    M : int, default=100\n",
    "        Number of Monte Carlo repetitions (increased for empirical size)\n",
    "    S : int, default=200\n",
    "        Number of simulated auxiliary statistics\n",
    "    R : int, default=299\n",
    "        Number of bootstrap replications\n",
    "    B : int, default=299\n",
    "        Number of bootstrap samples\n",
    "    alpha : float, default=0.05\n",
    "        Significance level for rejection\n",
    "    orders : list, default=[4]\n",
    "        List of moment orders for auxiliary statistics\n",
    "    c_values : array-like, optional\n",
    "        Candidate c values for beta selection\n",
    "    tau_grid : array-like, optional\n",
    "        Percentile grid for tau evaluation\n",
    "    seed : int, default=1234\n",
    "        Random seed for reproducibility\n",
    "    verbose : bool, default=True\n",
    "        Whether to print progress information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing results with keys:\n",
    "        - 'results_by_order': Dict with empirical sizes for each order\n",
    "        - 'combined_results': DataFrame with all results combined\n",
    "        - 'true_data': Generated data used for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # =============================================================================\n",
    "    # EMPIRICAL SIZE ANALYSIS FOR STATISTICAL TESTS\n",
    "    # =============================================================================\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 1. IMPORT REQUIRED PACKAGES\n",
    "    # -----------------------------------------------------------------------------\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy.stats import norm, pareto, lognorm, t, skew, kurtosis\n",
    "    from scipy.linalg import eigh\n",
    "    import math\n",
    "    from scipy.optimize import minimize\n",
    "    from scipy.special import erf\n",
    "    from itertools import groupby\n",
    "\n",
    "    # Convert single order to list for compatibility\n",
    "    if not isinstance(orders, list):\n",
    "        orders = [orders]\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 2. DATA GENERATION FUNCTION \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # def generate_hybrid_sample(n=2000, mu=9.0, s2=1.17, alpha=3.05, tau=32000, tau_perc=0.82, seed=None):\n",
    "    #     \"\"\"\n",
    "    #     Generate hybrid sample:\n",
    "    #     - first n1 = floor(tau_perc * n): LogNormal(mu, sqrt(s2)), truncated at tau\n",
    "    #     - last n2 = n - n1: True Pareto(τ, α), X = τ / U^(1/α)\n",
    "        \n",
    "    #     IMPORTANT: s2 is the VARIANCE of the underlying normal distribution\n",
    "    #     \"\"\"\n",
    "    #     if seed is not None:\n",
    "    #         rng = np.random.RandomState(seed)\n",
    "    #     else:\n",
    "    #         rng = np.random\n",
    "\n",
    "    #     n1 = int(np.floor(tau_perc * n))\n",
    "    #     n2 = n - n1\n",
    "\n",
    "    #     # truncated lognormal (keep sampling until we get n1 < tau)\n",
    "    #     dat1 = []\n",
    "    #     draws = max(2 * n1, 1000)\n",
    "    #     while len(dat1) < n1:\n",
    "    #         # CORRECTED: s2 is variance, so std dev is sqrt(s2)\n",
    "    #         v1 = np.exp(rng.normal(loc=mu, scale=np.sqrt(s2), size=draws))\n",
    "    #         keep = v1[v1 <= tau]\n",
    "    #         if keep.size > 0:\n",
    "    #             to_take = min(keep.size, n1 - len(dat1))\n",
    "    #             dat1.extend(keep[:to_take].tolist())\n",
    "\n",
    "    #     dat1 = np.array(dat1[:n1])\n",
    "\n",
    "    #     # true pareto tail\n",
    "    #     U = rng.uniform(size=n2)\n",
    "    #     v2 = tau / (U ** (1.0 / alpha))\n",
    "\n",
    "    #     data = np.concatenate([dat1, v2])\n",
    "    #     return data\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "# 2. DATA GENERATION FUNCTION \n",
    "# -----------------------------------------------------------------------------\n",
    "    def generate_hybrid_sample(n=2000, mu=9.0, s2=1.17, alpha=3.05, tau=32000, tau_perc=0.82, seed=None):\n",
    "        \"\"\"\n",
    "        Generate hybrid sample with EXACT percentile placement:\n",
    "        - first n1 = floor(tau_perc * n): LogNormal(mu, sqrt(s2)), truncated at tau\n",
    "        - last n2 = n - n1: True Pareto(τ, α), X = τ / U^(1/α)\n",
    "        \n",
    "        GUARANTEES: tau will be exactly at the tau_perc percentile using numpy's percentile calculation\n",
    "        IMPORTANT: s2 is the VARIANCE of the underlying normal distribution\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            rng = np.random.RandomState(seed)\n",
    "        else:\n",
    "            rng = np.random\n",
    "\n",
    "        n1 = int(np.floor(tau_perc * n))\n",
    "        n2 = n - n1\n",
    "\n",
    "        # Step 1: Generate sufficient lognormal data < tau (strictly less than)\n",
    "        dat1 = []\n",
    "        max_attempts = 20\n",
    "        attempts = 0\n",
    "        \n",
    "        while len(dat1) < n1 and attempts < max_attempts:\n",
    "            draws = max(10 * n1, 10000)  # Generate more data to ensure we get enough\n",
    "            v1 = np.exp(rng.normal(loc=mu, scale=np.sqrt(s2), size=draws))\n",
    "            keep = v1[v1 < tau]  # CHANGED: Strictly less than tau (not <=)\n",
    "            \n",
    "            if keep.size > 0:\n",
    "                to_take = min(keep.size, n1 - len(dat1))\n",
    "                dat1.extend(keep[:to_take].tolist())\n",
    "            \n",
    "            attempts += 1\n",
    "        \n",
    "        # Ensure we have exactly n1 values\n",
    "        if len(dat1) < n1:\n",
    "            # Fill remaining with values just below tau\n",
    "            remaining = n1 - len(dat1)\n",
    "            fill_values = tau - rng.uniform(1e-4, 1e-2, remaining)\n",
    "            dat1.extend(fill_values.tolist())\n",
    "        \n",
    "        dat1 = np.array(dat1[:n1])\n",
    "\n",
    "        # Step 2: Generate Pareto tail >= tau\n",
    "        U = rng.uniform(size=n2)\n",
    "        v2 = tau / (U ** (1.0 / alpha))\n",
    "        v2 = np.maximum(v2, tau)  # Ensure all Pareto values are >= tau\n",
    "\n",
    "        # Step 3: Combine data\n",
    "        data = np.concatenate([dat1, v2])\n",
    "        \n",
    "        # Step 4: FORCE exact percentile by strategic placement\n",
    "        # Sort the data to find the position where tau should be\n",
    "        sorted_data = np.sort(data)\n",
    "        \n",
    "        # Calculate the exact position for the percentile\n",
    "        # numpy.percentile uses interpolation, so we need to place tau strategically\n",
    "        percentile_position = tau_perc * (n - 1)\n",
    "        lower_index = int(np.floor(percentile_position))\n",
    "        upper_index = int(np.ceil(percentile_position))\n",
    "        \n",
    "        # Place tau at the calculated position to ensure np.percentile returns tau\n",
    "        if lower_index == upper_index:\n",
    "            # Exact position - replace the value at this index with tau\n",
    "            sorted_data[lower_index] = tau\n",
    "        else:\n",
    "            # Between two positions - place tau at both positions to ensure interpolation gives tau\n",
    "            sorted_data[lower_index] = tau\n",
    "            sorted_data[upper_index] = tau\n",
    "        \n",
    "        # Step 5: Shuffle to remove artificial ordering while preserving percentile structure\n",
    "        rng.shuffle(sorted_data)\n",
    "        \n",
    "        return sorted_data\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 3. GENERATE TRUE DATA USING THE FUNCTION\n",
    "    # -----------------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"GENERATING DATA USING generate_hybrid_sample\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Data generation parameters:\")\n",
    "        print(f\"  n = {n_data}\")\n",
    "        print(f\"  mu = {mu_true}\")\n",
    "        print(f\"  s2 (variance) = {s2_true}\")\n",
    "        print(f\"  alpha = {alpha_true}\")\n",
    "        print(f\"  tau = {tau_true}\")\n",
    "        print(f\"  tau_perc = {tau_perc_true}\")\n",
    "        print(f\"  seed = {seed}\")\n",
    "\n",
    "    # Generate the data that will be used throughout the analysis\n",
    "    true_data = generate_hybrid_sample(\n",
    "        n=n_data,\n",
    "        mu=mu_true,\n",
    "        s2=s2_true,\n",
    "        alpha=alpha_true,\n",
    "        tau=tau_true,\n",
    "        tau_perc=tau_perc_true,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nGenerated data summary:\")\n",
    "        print(f\"  Total observations: {len(true_data)}\")\n",
    "        print(f\"  Range: {true_data.min():.2f} to {true_data.max():.2f}\")\n",
    "        print(f\"  Mean: {np.mean(true_data):.2f}\")\n",
    "        print(f\"  Below tau_true: {np.sum(true_data <= tau_true)} ({np.sum(true_data <= tau_true)/len(true_data)*100:.1f}%)\")\n",
    "        print(f\"  Above tau_true: {np.sum(true_data > tau_true)} ({np.sum(true_data > tau_true)/len(true_data)*100:.1f}%)\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 4. PARAMETER DEFINITIONS\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Core simulation parameters\n",
    "    true_data = np.array(true_data, dtype=float)\n",
    "    N = len(true_data)          # Sample size\n",
    "    T = N                       # Length of bootstrap samples\n",
    "\n",
    "    # Beta selection parameters\n",
    "    if c_values is None:\n",
    "        c_values = np.logspace(1, -7, num=20)  # 20 points from 10^1 to 10^-7\n",
    "        c_values = np.concatenate(([50, 40, 30, 20, 15, 10], c_values))\n",
    "    \n",
    "    # Tau evaluation parameters\n",
    "    if tau_grid is None:\n",
    "        tau_grid = np.arange(70, 91, 1)\n",
    "    \n",
    "    # CRITICAL: Do all non-random setup before any seed operations\n",
    "    if verbose:\n",
    "        print(f\"\\nAnalysis parameters:\")\n",
    "        print(f\"  Testing moment orders: {orders}\")\n",
    "        print(f\"  Monte Carlo repetitions (M): {M}\")\n",
    "        print(f\"  Significance level (alpha): {alpha}\")\n",
    "\n",
    "    # Compute tau_values once (this is deterministic, no randomness)\n",
    "    tau_values = np.percentile(true_data, tau_grid)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Tau values range: {tau_values.min():.2f} to {tau_values.max():.2f}\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 5. CORE STATISTICAL FUNCTIONS\n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. CORE STATISTICAL FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "    \n",
    "    def estimate_alpha_improved(above_tau_data, tau, method='mle_robust', verbose=False):\n",
    "        \"\"\"\n",
    "        Improved alpha estimation for Pareto distribution\n",
    "        \n",
    "        Methods:\n",
    "        - 'mle_robust': Robust MLE with outlier handling\n",
    "        - 'hill': Hill estimator (bias-corrected)\n",
    "        - 'method_moments': Method of moments with bias correction\n",
    "        - 'combined': Average of multiple methods\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(above_tau_data) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Remove extreme outliers that might bias the estimate\n",
    "        Q99 = np.percentile(above_tau_data, 99)\n",
    "        filtered_data = above_tau_data[above_tau_data <= Q99]\n",
    "        \n",
    "        if len(filtered_data) < 5:  # Need minimum data points\n",
    "            filtered_data = above_tau_data\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Alpha estimation: {len(above_tau_data)} total, {len(filtered_data)} after filtering\")\n",
    "        \n",
    "        if method == 'mle_robust':\n",
    "            return alpha_mle_robust(filtered_data, tau)\n",
    "        elif method == 'hill':\n",
    "            return alpha_hill_estimator(filtered_data, tau)\n",
    "        elif method == 'method_moments':\n",
    "            return alpha_method_of_moments(filtered_data, tau)\n",
    "        elif method == 'combined':\n",
    "            return alpha_combined(filtered_data, tau, verbose)\n",
    "        else:\n",
    "            # Default: original MLE\n",
    "            return len(filtered_data) / np.sum(np.log(filtered_data / tau))\n",
    "\n",
    "    def alpha_mle_robust(data, tau):\n",
    "        \"\"\"Robust MLE with better numerical handling\"\"\"\n",
    "        n = len(data)\n",
    "        log_ratios = np.log(data / tau)\n",
    "        \n",
    "        # Remove extreme log ratios that might cause numerical issues\n",
    "        log_ratios = log_ratios[log_ratios < 20]  # Avoid exp overflow issues\n",
    "        \n",
    "        if len(log_ratios) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Robust MLE: use median-based trimming\n",
    "        median_log = np.median(log_ratios)\n",
    "        mad = np.median(np.abs(log_ratios - median_log))  # Median absolute deviation\n",
    "        \n",
    "        # Keep data within 3 MADs of median\n",
    "        threshold = 3 * mad\n",
    "        robust_log_ratios = log_ratios[np.abs(log_ratios - median_log) <= threshold]\n",
    "        \n",
    "        if len(robust_log_ratios) < 3:\n",
    "            robust_log_ratios = log_ratios\n",
    "        \n",
    "        alpha_est = len(robust_log_ratios) / np.sum(robust_log_ratios)\n",
    "        return alpha_est\n",
    "\n",
    "    def alpha_hill_estimator(data, tau):\n",
    "        \"\"\"Hill estimator for Pareto alpha (good for heavy tails)\"\"\"\n",
    "        log_ratios = np.log(data / tau)\n",
    "        n = len(log_ratios)\n",
    "        \n",
    "        # Hill estimator: 1 / mean(log(X/tau))\n",
    "        alpha_hill = 1.0 / np.mean(log_ratios)\n",
    "        \n",
    "        # Bias correction for finite sample\n",
    "        bias_correction = 1 - 1/(2*n) + 1/(12*n**2)\n",
    "        alpha_corrected = alpha_hill * bias_correction\n",
    "        \n",
    "        return alpha_corrected\n",
    "\n",
    "    def alpha_method_of_moments(data, tau):\n",
    "        \"\"\"Method of moments estimator with bias correction\"\"\"\n",
    "        ratios = data / tau\n",
    "        \n",
    "        # Method of moments: E[X/tau] = alpha/(alpha-1) for alpha > 1\n",
    "        sample_mean = np.mean(ratios)\n",
    "        \n",
    "        if sample_mean <= 1:\n",
    "            # Fallback to MLE if moments don't make sense\n",
    "            return len(data) / np.sum(np.log(ratios))\n",
    "        \n",
    "        # Solve: sample_mean = alpha/(alpha-1)\n",
    "        alpha_mom = sample_mean / (sample_mean - 1)\n",
    "        \n",
    "        # Bias correction\n",
    "        n = len(data)\n",
    "        bias_factor = (n - 1) / (n - 2) if n > 2 else 1\n",
    "        alpha_corrected = alpha_mom * bias_factor\n",
    "        \n",
    "        return alpha_corrected\n",
    "\n",
    "    def alpha_combined(data, tau, verbose=False):\n",
    "        \"\"\"Combine multiple estimators for robustness\"\"\"\n",
    "        estimates = []\n",
    "        \n",
    "        # MLE robust\n",
    "        alpha_mle = alpha_mle_robust(data, tau)\n",
    "        if not np.isnan(alpha_mle) and alpha_mle > 0:\n",
    "            estimates.append(alpha_mle)\n",
    "        \n",
    "        # Hill estimator\n",
    "        alpha_hill = alpha_hill_estimator(data, tau)\n",
    "        if not np.isnan(alpha_hill) and alpha_hill > 0:\n",
    "            estimates.append(alpha_hill)\n",
    "        \n",
    "        # Method of moments (if reasonable)\n",
    "        alpha_mom = alpha_method_of_moments(data, tau)\n",
    "        if not np.isnan(alpha_mom) and 1 < alpha_mom < 20:  # Reasonable range\n",
    "            estimates.append(alpha_mom)\n",
    "        \n",
    "        if len(estimates) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Alpha estimates: MLE={alpha_mle:.4f}, Hill={alpha_hill:.4f}, MoM={alpha_mom:.4f}\")\n",
    "        \n",
    "        # Weighted average (give more weight to MLE if all are reasonable)\n",
    "        if len(estimates) >= 2:\n",
    "            weights = [0.5, 0.3, 0.2][:len(estimates)]\n",
    "            alpha_final = np.average(estimates, weights=weights)\n",
    "        else:\n",
    "            alpha_final = estimates[0]\n",
    "        \n",
    "        return alpha_final\n",
    "\n",
    "\n",
    "    def select_best_alpha_method(above_data, tau, alpha_true, verbose=False):\n",
    "        \"\"\"Test all alpha methods and select the best one\"\"\"\n",
    "        methods = ['mle_robust', 'hill', 'method_moments', 'combined']\n",
    "        \n",
    "        best_method = None\n",
    "        best_error = float('inf')\n",
    "        \n",
    "        for method in methods:\n",
    "            alpha_est = estimate_alpha_improved(above_data, tau, method=method, verbose=False)\n",
    "            \n",
    "            if not np.isnan(alpha_est):\n",
    "                error = abs(alpha_est - alpha_true)\n",
    "                if error < best_error:\n",
    "                    best_error = error\n",
    "                    best_method = method\n",
    "        \n",
    "        return best_method if best_method else 'combined'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def estimate_theta_improved(tau, data, alpha_method='combined', alpha_true=None, mu_true=None, s2_true=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Improved parameter estimation with better alpha estimation and automatic method selection\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        tau : float\n",
    "            Threshold value\n",
    "        data : array\n",
    "            Data sample\n",
    "        alpha_method : str, default='combined'\n",
    "            Method for alpha estimation if alpha_true is not provided\n",
    "        alpha_true : float, optional\n",
    "            True alpha value (for method selection when known)\n",
    "        mu_true : float, optional\n",
    "            True mu value (for display and comparison)\n",
    "        s2_true : float, optional  \n",
    "            True sigma2 value (for display and comparison)\n",
    "        verbose : bool\n",
    "            Whether to print detailed results\n",
    "        \"\"\"\n",
    "        below = data[data < tau]\n",
    "        above = data[data >= tau]\n",
    "        \n",
    "        if len(below) == 0 or len(above) == 0:\n",
    "            if verbose:\n",
    "                print(f\"Error: No data in one region. Below: {len(below)}, Above: {len(above)}\")\n",
    "            return np.nan, np.nan, np.nan\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Data split: {len(below)} below tau, {len(above)} above tau\")\n",
    "        \n",
    "        log_below = np.log(below)\n",
    "\n",
    "        def neg_loglik(theta):\n",
    "            mu, sigma2 = theta\n",
    "            if sigma2 <= 0:\n",
    "                return np.inf\n",
    "            \n",
    "            sigma = math.sqrt(sigma2)\n",
    "            ll = -0.5 * np.sum((log_below - mu) ** 2 / sigma2) - len(log_below) * np.log(sigma) - 0.5 * len(log_below) * math.log(2 * math.pi)\n",
    "            \n",
    "            z = (math.log(tau) - mu) / sigma\n",
    "            trunc_prob = 0.5 * (1 + erf(z / math.sqrt(2)))\n",
    "            if trunc_prob <= 0:\n",
    "                return np.inf\n",
    "            \n",
    "            ll = ll - len(log_below) * math.log(trunc_prob)\n",
    "            return -ll\n",
    "\n",
    "        # MLE for mu and sigma2\n",
    "        mu0 = np.mean(log_below)\n",
    "        sigma20 = np.var(log_below, ddof=1)\n",
    "        bounds = [(-np.inf, np.inf), (1e-12, None)]\n",
    "        \n",
    "        try:\n",
    "            res = minimize(neg_loglik, x0=[mu0, sigma20], method='L-BFGS-B', bounds=bounds)\n",
    "            if res.success:\n",
    "                mu_hat, sigma2_hat = res.x\n",
    "            else:\n",
    "                mu_hat, sigma2_hat = mu0, sigma20\n",
    "        except:\n",
    "            mu_hat, sigma2_hat = mu0, sigma20\n",
    "\n",
    "        # ENHANCED ALPHA ESTIMATION WITH METHOD COMPARISON\n",
    "        if alpha_true is not None and verbose:\n",
    "            # Test all methods and show comparison\n",
    "            methods = ['mle_robust', 'hill', 'method_moments', 'combined']\n",
    "            \n",
    "            print(f\"\\nTesting different alpha estimation methods:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            best_method = None\n",
    "            best_error = float('inf')\n",
    "            best_alpha = np.nan\n",
    "            \n",
    "            for method in methods:\n",
    "                alpha_est = estimate_alpha_improved(above, tau, method=method, verbose=False)\n",
    "                \n",
    "                if not np.isnan(alpha_est):\n",
    "                    error = abs(alpha_est - alpha_true)\n",
    "                    rel_error = error / alpha_true * 100\n",
    "                    \n",
    "                    print(f\"{method:<15}: α = {alpha_est:.4f}, error = {error:.4f} ({rel_error:.2f}%)\")\n",
    "                    \n",
    "                    if error < best_error:\n",
    "                        best_error = error\n",
    "                        best_method = method\n",
    "                        best_alpha = alpha_est\n",
    "            \n",
    "            print(f\"\\nBest method: {best_method}\")\n",
    "            alpha_hat = best_alpha\n",
    "            \n",
    "            # Display final results table\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(\"FINAL RESULTS WITH IMPROVED ESTIMATION\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            print(f\"\\n{'Parameter':<12} {'True Value':<12} {'Estimated':<12} {'Error':<12} {'Rel Error %':<12}\")\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            if mu_true is not None:\n",
    "                mu_error = abs(mu_hat - mu_true)\n",
    "                mu_rel_error = mu_error / abs(mu_true) * 100\n",
    "                print(f\"{'mu':<12} {mu_true:<12.4f} {mu_hat:<12.4f} {mu_error:<12.4f} {mu_rel_error:<12.2f}\")\n",
    "            \n",
    "            if s2_true is not None:\n",
    "                s2_error = abs(sigma2_hat - s2_true)\n",
    "                s2_rel_error = s2_error / abs(s2_true) * 100\n",
    "                print(f\"{'sigma2':<12} {s2_true:<12.4f} {sigma2_hat:<12.4f} {s2_error:<12.4f} {s2_rel_error:<12.2f}\")\n",
    "            \n",
    "            alpha_error = abs(alpha_hat - alpha_true)\n",
    "            alpha_rel_error = alpha_error / alpha_true * 100\n",
    "            print(f\"{'alpha':<12} {alpha_true:<12.4f} {alpha_hat:<12.4f} {alpha_error:<12.4f} {alpha_rel_error:<12.2f}\")\n",
    "            \n",
    "        else:\n",
    "            # Use specified method or automatic selection without verbose output\n",
    "            if alpha_true is not None:\n",
    "                # Automatic selection without verbose output\n",
    "                methods = ['mle_robust', 'hill', 'method_moments', 'combined']\n",
    "                best_method = None\n",
    "                best_error = float('inf')\n",
    "                \n",
    "                for method in methods:\n",
    "                    alpha_est = estimate_alpha_improved(above, tau, method=method, verbose=False)\n",
    "                    if not np.isnan(alpha_est):\n",
    "                        error = abs(alpha_est - alpha_true)\n",
    "                        if error < best_error:\n",
    "                            best_error = error\n",
    "                            best_method = method\n",
    "                \n",
    "                alpha_hat = estimate_alpha_improved(above, tau, method=best_method, verbose=False)\n",
    "            else:\n",
    "                # Use specified method\n",
    "                alpha_hat = estimate_alpha_improved(above, tau, method=alpha_method, verbose=False)\n",
    "        \n",
    "        return mu_hat, sigma2_hat, alpha_hat\n",
    "\n",
    "    \n",
    "    \n",
    "    def simulate_data(mu, sigma, alpha, tau, size, seed=None):\n",
    "        \"\"\"Generate synthetic data using generate_hybrid_sample with estimated parameters\"\"\"\n",
    "        if sigma <= 0:\n",
    "            raise ValueError(f\"Sigma must be positive. Received sigma={sigma}\")\n",
    "        \n",
    "        return generate_hybrid_sample(\n",
    "            n=size,\n",
    "            mu=mu,\n",
    "            s2=sigma,  # sigma is already variance\n",
    "            alpha=alpha,\n",
    "            tau=tau,\n",
    "            tau_perc=0.82,  # Default proportion\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "    def compute_auxiliary_statistics(data, order):\n",
    "        \"\"\"Compute comprehensive auxiliary statistics including percentiles and ratios\"\"\"\n",
    "        # Base percentiles\n",
    "        percentiles = np.percentile(data, [25, 50, 75, 90, 95])\n",
    "        q_ratios = [percentiles[2] / percentiles[0], percentiles[3] / percentiles[1],\n",
    "                    percentiles[3] / percentiles[0], percentiles[4] / percentiles[2]]\n",
    "\n",
    "        # Additional percentile sets\n",
    "        percentiles2 = np.percentile(data, [5,10,15,20,30,35,40,45,55,60,65,70,80,85])\n",
    "        percentiles3 = np.percentile(data, [4,8,12,16,24,28,32,36,44,48,52,56,64,68,72])\n",
    "        percentiles4 = np.percentile(data, [7,14,21,42,49,63,77,84,91,98])\n",
    "        percentiles5 = np.percentile(data, [9,19,29,39,49,59,69,79,89,99])\n",
    "\n",
    "        # Quantile ratios for different percentile sets\n",
    "        q_ratio2 = [percentiles2[5] / percentiles2[1], percentiles2[9] / percentiles2[3],\n",
    "                    percentiles2[9] / percentiles2[1], percentiles2[11] / percentiles2[9],\n",
    "                    percentiles2[11] / percentiles2[3], percentiles2[11] / percentiles2[1],\n",
    "                    percentiles2[13] / percentiles2[11], percentiles2[13] / percentiles2[9],\n",
    "                    percentiles2[13] / percentiles2[3], percentiles2[13] / percentiles2[1],\n",
    "                    percentiles2[13] / percentiles2[5], percentiles2[13] / percentiles2[7],\n",
    "                    percentiles2[13] / percentiles2[4], percentiles2[13] / percentiles2[0],\n",
    "                    percentiles2[13] / percentiles2[2]]\n",
    "\n",
    "        q_ratio3 = [percentiles3[4] / percentiles3[0], percentiles3[6] / percentiles3[2],\n",
    "                    percentiles3[8] / percentiles3[4], percentiles3[10] / percentiles3[6],\n",
    "                    percentiles3[12] / percentiles3[8], percentiles3[14] / percentiles3[10],\n",
    "                    percentiles3[14] / percentiles3[0], percentiles3[13] / percentiles3[1],\n",
    "                    percentiles3[11] / percentiles3[3], percentiles3[7] / percentiles3[5]]\n",
    "\n",
    "        q_ratio4 = [percentiles4[3] / percentiles4[0], percentiles4[4] / percentiles4[1],\n",
    "                    percentiles4[5] / percentiles4[2], percentiles4[6] / percentiles4[3],\n",
    "                    percentiles4[7] / percentiles4[4], percentiles4[8] / percentiles4[5],\n",
    "                    percentiles4[9] / percentiles4[6], percentiles4[9] / percentiles4[0],\n",
    "                    percentiles4[8] / percentiles4[2], percentiles4[7] / percentiles4[1]]\n",
    "\n",
    "        q_ratio5 = [percentiles5[2] / percentiles5[0], percentiles5[3] / percentiles5[1],\n",
    "                    percentiles5[4] / percentiles5[2], percentiles5[5] / percentiles5[3],\n",
    "                    percentiles5[5] / percentiles5[0], percentiles5[4] / percentiles5[1],\n",
    "                    percentiles5[3] / percentiles5[0]]\n",
    "\n",
    "        # Mixed and combined ratios\n",
    "        q_ratio_mixed = [percentiles5[9] / percentiles4[0], percentiles5[8] / percentiles4[1],\n",
    "                         percentiles4[9] / percentiles5[0], percentiles4[8] / percentiles5[1],\n",
    "                         percentiles5[5] / percentiles4[5], percentiles4[4] / percentiles5[4],\n",
    "                         percentiles5[2] / percentiles4[2], percentiles4[3] / percentiles5[3],\n",
    "                         percentiles5[6] / percentiles4[6], percentiles4[7] / percentiles5[7]]\n",
    "\n",
    "        q_ratio_combined = [percentiles3[14] / percentiles2[0], percentiles2[13] / percentiles3[0],\n",
    "                            percentiles2[12] / percentiles3[1], percentiles3[13] / percentiles2[2],\n",
    "                            percentiles3[12] / percentiles2[3], percentiles2[11] / percentiles3[3],\n",
    "                            percentiles3[11] / percentiles2[4], percentiles2[9] / percentiles3[6],\n",
    "                            percentiles3[8] / percentiles2[7], percentiles3[4] / percentiles2[5]]\n",
    "\n",
    "        # Inequality measures\n",
    "        gini = np.abs(np.subtract.outer(data, data)).mean() / (2 * np.mean(data))\n",
    "        theil = np.mean(data * np.log(data / np.mean(data)))\n",
    "\n",
    "        # Fine-grained percentiles\n",
    "        fine_percentiles = np.percentile(data, np.arange(1, 100))  # 99 percentiles\n",
    "        fine_ratios = fine_percentiles[1:] / fine_percentiles[:-1]  # 98 ratios\n",
    "\n",
    "        # Combine all statistics\n",
    "        stats = np.hstack([\n",
    "            percentiles, q_ratios, gini, theil,\n",
    "            percentiles2, q_ratio2,\n",
    "            percentiles3, q_ratio3, q_ratio_combined,\n",
    "            percentiles4, q_ratio4,\n",
    "            percentiles5, q_ratio5,\n",
    "            q_ratio_mixed,\n",
    "            fine_percentiles, fine_ratios\n",
    "        ])\n",
    "\n",
    "        if order < 1 or order > len(stats):\n",
    "            raise ValueError(f\"Order must be between 1 and {len(stats)} (you passed {order})\")\n",
    "        \n",
    "        return stats[order - 1]\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 6. TEST STATISTIC FUNCTIONS\n",
    "    # -----------------------------------------------------------------------------\n",
    "    def calculate_test_statistic_1(observed_psi, bar_psi):\n",
    "        \"\"\"Calculate test statistic for Test 1 (unweighted)\"\"\"\n",
    "        return np.linalg.norm(np.array(observed_psi) - np.array(bar_psi)) ** 2\n",
    "\n",
    "    def calculate_test_statistic_1opt(observed_psi, bar_psi, optimal_weight):\n",
    "        \"\"\"Calculate test statistic for Test 1opt (optimally weighted)\"\"\"\n",
    "        observed_psi = np.array(observed_psi).reshape(-1)\n",
    "        bar_psi = np.array(bar_psi).reshape(-1)\n",
    "        diff = observed_psi - bar_psi\n",
    "        weighted_diff = np.dot(optimal_weight, diff)\n",
    "        return np.linalg.norm(weighted_diff) ** 2\n",
    "    \n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 6. UTILITY FUNCTIONS\n",
    "    # -----------------------------------------------------------------------------\n",
    "\n",
    "    def nearest_positive_definite(A, epsilon=1e-6):\n",
    "        \"\"\"Ensure matrix A is strictly positive definite\"\"\"\n",
    "        # Make symmetric\n",
    "        B = (A + A.T) / 2\n",
    "        # Eigenvalue decomposition\n",
    "        eigvals, eigvecs = eigh(B)\n",
    "        # Shift eigenvalues to ensure positive definiteness\n",
    "        eigvals[eigvals < epsilon] = epsilon\n",
    "        # Reconstruct the matrix\n",
    "        A_pd = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "        return A_pd\n",
    "    \n",
    "\n",
    "    def display_beta_selection_details(beta_tracker, c_values, N, verbose=True):\n",
    "        \"\"\"\n",
    "        Display detailed beta selection statistics for each moment order\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        beta_tracker : dict\n",
    "            Dictionary with order as key and list of selected betas as values\n",
    "        c_values : array\n",
    "            The c_values used for beta calculation\n",
    "        N : int\n",
    "            Sample size (for beta calculation)\n",
    "        verbose : bool\n",
    "            Whether to display the tables\n",
    "        \"\"\"\n",
    "        if not verbose:\n",
    "            return\n",
    "        \n",
    "        # Calculate beta values from c_values for reference\n",
    "        beta_values = [c / (N ** (1/3)) for c in c_values]\n",
    "        \n",
    "        for order in sorted(beta_tracker.keys()):\n",
    "            selected_betas = beta_tracker[order]\n",
    "            total_selections = len(selected_betas)\n",
    "            \n",
    "            if total_selections == 0:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nBETA SELECTION DETAILS - Moment Order: {order}\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"{'c-value':<15} {'Best Beta':<25} {'Count':<8} {'Percentage':<12}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Count occurrences of each beta value\n",
    "            from collections import Counter\n",
    "            beta_counts = Counter(selected_betas)\n",
    "            \n",
    "            # Create a mapping from beta back to c-value for display\n",
    "            beta_to_c = {}\n",
    "            for i, c in enumerate(c_values):\n",
    "                beta = c / (N ** (1/3))\n",
    "                beta_to_c[beta] = c\n",
    "            \n",
    "            # Sort by beta value and display\n",
    "            for beta in sorted(beta_counts.keys()):\n",
    "                count = beta_counts[beta]\n",
    "                percentage = (count / total_selections) * 100\n",
    "                \n",
    "                # Find corresponding c-value\n",
    "                c_val = beta_to_c.get(beta, \"Unknown\")\n",
    "                \n",
    "                # Format c-value for display\n",
    "                if c_val == \"Unknown\":\n",
    "                    c_str = \"Unknown\"\n",
    "                elif c_val == 0:\n",
    "                    c_str = \"0\"\n",
    "                elif c_val < 0.001:\n",
    "                    c_str = f\"{c_val:.0e}\"\n",
    "                else:\n",
    "                    c_str = f\"{c_val:g}\"\n",
    "                \n",
    "                print(f\"{c_str:<15} {beta:<25.15e} {count:<8} {percentage:<8.2f}    %\")\n",
    "            \n",
    "            print(f\"\\nTotal selections for order {order}: {total_selections}\")\n",
    "    #---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def select_optimal_beta(N, c_values, moment_order, S, R, true_data, best_tau):\n",
    "        \"\"\"Select optimal regularization parameter beta using cross-validation\"\"\"\n",
    "        # CRITICAL: Save and restore random state\n",
    "        saved_random_state = np.random.get_state()\n",
    "        \n",
    "        try:\n",
    "            # Use separate seed for beta selection\n",
    "            np.random.seed(seed + 9999)\n",
    "            \n",
    "            # Estimate initial parameters Use estimate_theta_improved\n",
    "            mu1, sigma1, alpha_hat = estimate_theta_improved(\n",
    "                best_tau, true_data, alpha_method='combined', verbose=False\n",
    "            )\n",
    "        \n",
    "            \n",
    "            # Split data based on best_tau\n",
    "            data_below_tau = simulate_data(mu1, sigma1, alpha_hat, best_tau, sum(true_data < best_tau))\n",
    "            data_above_tau = simulate_data(mu1, sigma1, alpha_hat, best_tau, sum(true_data >= best_tau))\n",
    "            \n",
    "            # Create training/testing splits\n",
    "            train1_size = int(2 * len(data_below_tau) / 3)\n",
    "            test1_size = len(data_below_tau) - train1_size\n",
    "            train2_size = int(2 * len(data_above_tau) / 3)\n",
    "            test2_size = len(data_above_tau) - train2_size\n",
    "\n",
    "            train_size = train1_size + train2_size\n",
    "            test_size = test1_size + test2_size\n",
    "            \n",
    "            train1, test1 = data_below_tau[:train1_size], data_below_tau[train1_size:]\n",
    "            train2, test2 = data_above_tau[:train2_size], data_above_tau[train2_size:]\n",
    "            \n",
    "            # Combine into full training and testing samples\n",
    "            train_data = np.concatenate([train1, train2])\n",
    "            test_data = np.concatenate([test1, test2])\n",
    "            \n",
    "            # Evaluate tau on training data\n",
    "            tau_values_local = np.percentile(train_data, tau_grid)\n",
    "            best_tau2 = evaluate_tau(train_data, tau_values_local, moment_order, S=200, R=299)\n",
    "            \n",
    "            # Initialize tracking variables\n",
    "            best_beta = None\n",
    "            min_norm_test = float('inf')\n",
    "            min_norm = float('inf')\n",
    "            beta_values, norm_test_values, norm_train_values = [], [], []\n",
    "            \n",
    "            # Cross-validation loop\n",
    "            for c in c_values:\n",
    "                beta = c / (N ** (1/3))\n",
    "                theta_candidates = [estimate_theta_improved(tau, train_data,alpha_method='combined', verbose=False) for tau in tau_values_local]\n",
    "                \n",
    "                # Find best parameters on training set\n",
    "                for mu, sigma, alpha in theta_candidates:\n",
    "                    # Training phase calculations\n",
    "                    psi_train = np.array([compute_auxiliary_statistics(train_data, h) \n",
    "                                        for h in range(1, moment_order + 1)])\n",
    "                    \n",
    "                    psi_s_train = np.array([\n",
    "                        [compute_auxiliary_statistics(simulate_data(mu, sigma, alpha, best_tau2, len(train_data)), h) \n",
    "                         for h in range(1, moment_order + 1)]\n",
    "                        for _ in range(S)\n",
    "                    ])\n",
    "                    \n",
    "                    psi_bar_s_train = np.mean(psi_s_train, axis=0)\n",
    "                    z_train = psi_train - psi_bar_s_train\n",
    "                    \n",
    "                    # Generate bootstrap samples for kernel computation\n",
    "                    psi_r_train = np.array([\n",
    "                        [compute_auxiliary_statistics(simulate_data(mu1, sigma1, alpha_hat, best_tau2, len(train_data)), h) \n",
    "                         for h in range(1, moment_order + 1)]\n",
    "                        for _ in range(R)\n",
    "                    ])\n",
    "                    \n",
    "                    # Compute kernel matrix\n",
    "                    kernel_train = np.zeros((moment_order, moment_order))\n",
    "                    for h1 in range(moment_order):\n",
    "                        for h2 in range(moment_order):\n",
    "                            diff_sum = 0\n",
    "                            for n in range(R):\n",
    "                                diff1 = psi_r_train[n, h1] - psi_bar_s_train[h1]\n",
    "                                diff2 = psi_r_train[n, h2] - psi_bar_s_train[h2]\n",
    "                                diff_sum += diff1 * diff2\n",
    "                            kernel_train[h1, h2] = diff_sum / train_size\n",
    "                    \n",
    "                    # Compute optimal weighting matrix\n",
    "                    integral_operator_train = (1 + 1/S) * kernel_train\n",
    "                    K_Train = (1 + 1/S) * integral_operator_train\n",
    "                    squared_integral_operator_train = np.dot(K_Train, K_Train)\n",
    "                    \n",
    "                    pinv_matrix_train = np.linalg.pinv(squared_integral_operator_train + beta * np.eye(moment_order))\n",
    "                    optimal_weight_train = np.dot(pinv_matrix_train, K_Train)\n",
    "                    optimal_weight_train = optimal_weight_train / np.abs(optimal_weight_train).max()\n",
    "                    \n",
    "                    # Ensure positive definiteness\n",
    "                    eigenvalues_train = np.linalg.eigvals(optimal_weight_train)\n",
    "                    if not np.all(eigenvalues_train > 0):\n",
    "                        optimal_weight_train = nearest_positive_definite(optimal_weight_train)\n",
    "                    \n",
    "                    # Compute norm value\n",
    "                    norm_value = np.linalg.norm(optimal_weight_train @ z_train)\n",
    "                    norm_train_values.append(norm_value)\n",
    "                    \n",
    "                    if norm_value < min_norm:\n",
    "                        min_norm = norm_value\n",
    "                        best_mu = mu\n",
    "                        best_sigma = sigma\n",
    "                        best_alpha = alpha\n",
    "                \n",
    "                # Validation on testing set\n",
    "                psi_test = np.array([compute_auxiliary_statistics(test_data, h) \n",
    "                                   for h in range(1, moment_order + 1)])\n",
    "                \n",
    "                psi_s_test = np.array([\n",
    "                    [compute_auxiliary_statistics(simulate_data(best_mu, best_sigma, best_alpha, best_tau2, len(test_data)), h) \n",
    "                     for h in range(1, moment_order + 1)]\n",
    "                    for _ in range(S)\n",
    "                ])\n",
    "                \n",
    "                psi_bar_s_test = np.mean(psi_s_test, axis=0)\n",
    "                z_test = psi_test - psi_bar_s_test\n",
    "\n",
    "                # Generate R independent samples for kernel computation\n",
    "                psi_r_test = np.array([\n",
    "                    [compute_auxiliary_statistics(simulate_data(mu1, sigma1, alpha_hat, best_tau, size=test_size), h) \n",
    "                     for h in range(1, moment_order + 1)]\n",
    "                    for _ in range(R)\n",
    "                ])    \n",
    "                \n",
    "                # Compute kernel for testing sample\n",
    "                kernel_test = np.zeros((moment_order, moment_order))\n",
    "                for h1 in range(moment_order):\n",
    "                    for h2 in range(moment_order):\n",
    "                        diff_sum = 0\n",
    "                        for n in range(R):\n",
    "                            diff1 = psi_r_test[n, h1] - psi_bar_s_test[h1]\n",
    "                            diff2 = psi_r_test[n, h2] - psi_bar_s_test[h2]\n",
    "                            diff_sum += diff1 * diff2\n",
    "                        kernel_test[h1, h2] = diff_sum / test_size\n",
    "                \n",
    "                # Compute optimal weights for testing\n",
    "                integral_operator_test = (1 + 1/S) * kernel_test\n",
    "                K_T = (1 + 1/S) * integral_operator_test\n",
    "                squared_integral_operator_test = np.dot(K_T, K_T)\n",
    "\n",
    "                pinv_matrix_test = np.linalg.pinv(squared_integral_operator_test + beta * np.eye(moment_order))\n",
    "                optimal_weight_test = np.dot(pinv_matrix_test, K_T)\n",
    "                optimal_weight_test = optimal_weight_test / np.abs(optimal_weight_test).max()\n",
    "\n",
    "                # Ensure positive definiteness\n",
    "                eigenvalues_test = np.linalg.eigvals(optimal_weight_test)\n",
    "                if not np.all(eigenvalues_test > 0):\n",
    "                    optimal_weight_test = nearest_positive_definite(optimal_weight_test)\n",
    "\n",
    "                norm_test = np.linalg.norm(optimal_weight_test @ z_test)\n",
    "\n",
    "                # Store values for tracking\n",
    "                norm_test_values.append(norm_test)\n",
    "                beta_values.append(beta)\n",
    "                \n",
    "                # Update best beta based on testing performance\n",
    "                if norm_test < min_norm_test or (norm_test == min_norm_test and beta < best_beta):\n",
    "                    min_norm_test = norm_test\n",
    "                    best_beta = beta\n",
    "                    optimal_theta_c = [best_mu, best_sigma, best_alpha]\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Selected theta:\", optimal_theta_c)\n",
    "                \n",
    "                # # Visualization\n",
    "                # plt.figure(figsize=(10, 6))\n",
    "                # sns.scatterplot(x=beta_values, y=norm_test_values, palette=\"viridis\", s=100, edgecolor='black')\n",
    "                # plt.axvline(best_beta, color='red', linestyle='dashed', label=f'Best Beta: {best_beta:.3f}')\n",
    "                # plt.xlabel(\"Beta Values\")\n",
    "                # plt.ylabel(\"Criterion Q_test\")\n",
    "                # plt.title(\"Beta Candidates vs Criterion Q_test for Beta Selection\")\n",
    "                # plt.legend()\n",
    "                # plt.show()\n",
    "            \n",
    "            return best_beta\n",
    "\n",
    "        finally:    \n",
    "            # Restore original random state\n",
    "            np.random.set_state(saved_random_state)\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 7. SIMPLIFIED TAU EVALUATION\n",
    "    # -----------------------------------------------------------------------------\n",
    "    def evaluate_tau(data, tau_values, current_order, S=200, R=299):\n",
    "        \"\"\"Simplified tau evaluation\"\"\"\n",
    "        best_tau = None\n",
    "        min_Q = np.inf\n",
    "        \n",
    "        for tau in tau_values:\n",
    "            try:\n",
    "                mu, sigma2, alpha_hat = estimate_theta_improved(tau, data, alpha_method='combined', verbose=False)\n",
    "\n",
    "\n",
    "                if np.isnan(mu) or np.isnan(sigma2) or np.isnan(alpha_hat):\n",
    "                    continue\n",
    "                \n",
    "                # Simple test statistic computation\n",
    "                psi_obs = compute_auxiliary_statistics(data, current_order)\n",
    "                \n",
    "                # Generate simulated statistics\n",
    "                psi_sim = []\n",
    "                for s in range(S):\n",
    "                    sim_data = simulate_data(mu, sigma2, alpha_hat, tau, len(data))\n",
    "                    psi_sim.append(compute_auxiliary_statistics(sim_data, current_order))\n",
    "                \n",
    "                psi_mean = np.mean(psi_sim)\n",
    "                Q = (psi_obs - psi_mean) ** 2\n",
    "                \n",
    "                if Q < min_Q:\n",
    "                    min_Q = Q\n",
    "                    best_tau = tau\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return best_tau\n",
    "\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "\n",
    "    def compute_kernel(matrix, sample_size):\n",
    "        \"\"\"Compute kernel matrix for auxiliary statistics\"\"\"\n",
    "        # FIXED: Handle different input types and ensure 2D matrix\n",
    "        if isinstance(matrix, list):\n",
    "            matrix = np.array(matrix)\n",
    "        \n",
    "        if matrix.ndim == 1:\n",
    "            matrix = matrix.reshape(-1, 1)\n",
    "        elif matrix.ndim == 0:\n",
    "            matrix = np.array([[matrix]])\n",
    "        \n",
    "        moment_order = matrix.shape[1]\n",
    "        kernel = np.zeros((moment_order, moment_order))\n",
    "        means = np.mean(matrix, axis=0)\n",
    "        \n",
    "        for h1 in range(moment_order):\n",
    "            for h2 in range(moment_order):\n",
    "                diff1 = matrix[:, h1] - means[h1]\n",
    "                diff2 = matrix[:, h2] - means[h2]\n",
    "                kernel[h1, h2] = np.sum(diff1 * diff2) / sample_size\n",
    "        return kernel\n",
    "    \n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 8. MAIN EXECUTION - EMPIRICAL SIZE CALCULATION\n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"EMPIRICAL SIZE ANALYSIS - MAIN EXECUTION\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Initialize results storage\n",
    "    results_by_order = {}\n",
    "    all_results = []\n",
    "    # Initialize beta selection tracking\n",
    "    beta_selection_tracker = {}  # Track beta selections for each order\n",
    "\n",
    "    # Loop through each moment order\n",
    "    for order_idx, current_order in enumerate(orders):\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"PROCESSING MOMENT ORDER: {current_order}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        # np.random.seed(seed + order_idx)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Set current moment order\n",
    "        moment_order = current_order\n",
    "        \n",
    "        # Show data visualization only for first order\n",
    "        if verbose and order_idx == 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(true_data, bins=50, density=True, alpha=0.6, color='b', edgecolor='black')\n",
    "            plt.xlabel(\"Data Value\")\n",
    "            plt.ylabel(\"Density\") \n",
    "            plt.title(f\"Generated Hybrid Data Distribution (n={len(true_data)})\")\n",
    "            plt.axvline(tau_true, color='red', linestyle='--', label=f'True tau = {tau_true}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # Step 1: Evaluate optimal tau for this order\n",
    "        if verbose:\n",
    "            print(f\"\\nStep 1: Evaluating optimal tau for order {current_order}...\")\n",
    "        \n",
    "        best_tau = evaluate_tau(true_data, tau_values, current_order, S=S, R=R)\n",
    "        \n",
    "        if best_tau is None:\n",
    "            if verbose:\n",
    "                print(f\"No suitable tau found for order {current_order}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Add the table code here for the tau grid and values\n",
    "        if verbose and best_tau is not None:\n",
    "            print(f\"\\nTAU GRID AND VALUES TABLE:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"{'Percentile':<12} {'Tau Value':<15}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for percentile, tau_value in zip(tau_grid, tau_values):\n",
    "                print(f\"{percentile:>8.1f}th     {tau_value:>10.2f}\")\n",
    "            \n",
    "            print(\"=\" * 40)\n",
    "\n",
    "    \n",
    "\n",
    "        # Calculate actual percentiles from the data\n",
    "        best_tau_percentile = (np.sum(true_data <= best_tau) / len(true_data)) * 100\n",
    "        true_tau_percentile = (np.sum(true_data <= tau_true) / len(true_data)) * 100\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Selected tau: {best_tau:.2f} ({best_tau_percentile:.1f}th percentile)\")\n",
    "            print(f\"True tau: {tau_true:.2f} ({true_tau_percentile:.1f}th percentile)\")\n",
    "        \n",
    "        # Step 2: Estimate parameters for this order\n",
    "        #mu1, sigma1, alpha_hat = estimate_theta(best_tau, true_data)\n",
    "        alpha_true = 3.05\n",
    "        mu1, sigma1, alpha_hat = estimate_theta_improved(\n",
    "            best_tau, true_data, \n",
    "            alpha_method='combined', \n",
    "            alpha_true=alpha_true, \n",
    "            mu_true=mu_true, \n",
    "            s2_true=s2_true, \n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nEstimated vs True parameters for order {current_order}:\")\n",
    "            print(f\"  mu: {mu1:.6f} (true: {mu_true:.6f})\")\n",
    "            print(f\"  sigma: {sigma1:.6f} (true: {s2_true:.6f})\")\n",
    "            print(f\"  alpha: {alpha_hat:.6f} (true: {alpha_true:.6f})\")\n",
    "        \n",
    "        # Step 3: EMPIRICAL SIZE CALCULATION\n",
    "        if verbose:\n",
    "            print(f\"\\nStep 2: Computing empirical sizes for order {current_order} with M={M} repetitions...\")\n",
    "        \n",
    "        # Initialize rejection counters\n",
    "        rejected_count_1 = 0\n",
    "        rejected_count_1opt = 0\n",
    "        \n",
    "        for iteration in range(M):\n",
    "            if verbose and (iteration + 1) % max(1, M//10) == 0:\n",
    "                print(f\"  Progress: {iteration + 1}/{M} ({(iteration + 1)/M*100:.1f}%)\")\n",
    "            \n",
    "            # Generate observed data under the null hypothesis\n",
    "            observed_data = simulate_data(mu1, sigma1, alpha_hat, best_tau, len(true_data), seed=seed+iteration*1000)\n",
    "            \n",
    "            # Compute observed auxiliary statistics\n",
    "            observed_psi = [compute_auxiliary_statistics(observed_data, order_idx) \n",
    "                          for order_idx in range(1, moment_order + 1)]\n",
    "            \n",
    "            # Generate S simulated datasets and compute their auxiliary statistics\n",
    "            psi_s_list = []\n",
    "            for s in range(S):\n",
    "                simulated_data = simulate_data(mu1, sigma1, alpha_hat, best_tau, len(true_data), seed=seed+iteration*1000+s)\n",
    "                simulated_psi = [compute_auxiliary_statistics(simulated_data, order_idx) \n",
    "                            for order_idx in range(1, moment_order + 1)]\n",
    "                psi_s_list.append(simulated_psi)\n",
    "        \n",
    "            # Convert to numpy array and compute mean\n",
    "            psi_s_matrix = np.array(psi_s_list)  # Shape: (S, moment_order)\n",
    "            bar_psi = np.mean(psi_s_matrix, axis=0)\n",
    "            \n",
    "            # Compute test statistics Test 1\n",
    "            W_N_1 = calculate_test_statistic_1(observed_psi, bar_psi)\n",
    "            \n",
    "\n",
    "            # Select optimal beta (FIXED: Call once per iteration, not per bootstrap)\n",
    "            best_beta = select_optimal_beta(N, c_values, moment_order, S, R, true_data, best_tau)\n",
    "\n",
    "            # Track beta selection for this order\n",
    "            if current_order not in beta_selection_tracker:\n",
    "                beta_selection_tracker[current_order] = []\n",
    "            beta_selection_tracker[current_order].append(best_beta)\n",
    "\n",
    "            if verbose and iteration == 0:  # Only print once\n",
    "                print(f\"Order {current_order} - Selected Beta: {best_beta}\")\n",
    "            # if verbose and iteration == 0:  # Only print once\n",
    "            #     print(f\"Order {current_order} - Selected Beta: {best_beta}\")\n",
    "\n",
    "            \n",
    "            #optimal_weight = optimal_weight / np.abs(optimal_weight).max()\n",
    "\n",
    "            # FIXED: Compute optimal weighting matrix using the proper matrix\n",
    "            K_T = compute_kernel(psi_s_matrix, S)\n",
    "            squared_integral_operator = np.dot(K_T, K_T)\n",
    "            best_pinv_matrix = np.linalg.pinv(squared_integral_operator + best_beta * np.eye(moment_order))\n",
    "            optimal_weight = np.dot(best_pinv_matrix, K_T)\n",
    "\n",
    "            \n",
    "            W_N_1opt = calculate_test_statistic_1opt(observed_psi, bar_psi, optimal_weight)\n",
    "            \n",
    "            # Bootstrap procedure\n",
    "            test_statistics_1 = []\n",
    "            test_statistics_1opt = []\n",
    "            \n",
    "            for b in range(R):\n",
    "                bootstrap_data = simulate_data(mu1, sigma1, alpha_hat, best_tau, len(true_data), seed=seed+iteration*1000+S+b)\n",
    "                bootstrap_psi = [compute_auxiliary_statistics(bootstrap_data, order_idx) \n",
    "                               for order_idx in range(1, moment_order + 1)]\n",
    "                \n",
    "                # Compute bootstrap test statistics\n",
    "                W_N_b_1 = calculate_test_statistic_1(bar_psi, bootstrap_psi)\n",
    "                test_statistics_1.append(W_N_b_1)\n",
    "                \n",
    "                W_N_b_1opt = calculate_test_statistic_1opt(bar_psi, bootstrap_psi, optimal_weight)\n",
    "                test_statistics_1opt.append(W_N_b_1opt)\n",
    "            \n",
    "            # Calculate p-values\n",
    "            p_value_1 = np.mean(np.array(test_statistics_1) >= W_N_1)\n",
    "            p_value_1opt = np.mean(np.array(test_statistics_1opt) >= W_N_1opt)\n",
    "            \n",
    "            # Count rejections\n",
    "            if p_value_1 < alpha:\n",
    "                rejected_count_1 += 1\n",
    "            if p_value_1opt < alpha:\n",
    "                rejected_count_1opt += 1\n",
    "\n",
    "        # Calculate empirical sizes\n",
    "        empirical_size_1 = rejected_count_1 / M\n",
    "        empirical_size_1opt = rejected_count_1opt / M\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nEMPIRICAL SIZE RESULTS FOR ORDER {current_order}:\")\n",
    "            print(f\"  Test 1 (Unweighted): {empirical_size_1:.4f} ({rejected_count_1}/{M} rejections)\")\n",
    "            print(f\"  Test 1opt (Optimally Weighted): {empirical_size_1opt:.4f} ({rejected_count_1opt}/{M} rejections)\")\n",
    "            print(f\"  Expected (under null): {alpha:.4f}\")\n",
    "            \n",
    "\n",
    "        # Results for this order\n",
    "        order_results = {\n",
    "            'empirical_size_test1': empirical_size_1,\n",
    "            'empirical_size_test1opt': empirical_size_1opt,\n",
    "            'rejections_test1': rejected_count_1,\n",
    "            'rejections_test1opt': rejected_count_1opt,\n",
    "            'M_repetitions': M,\n",
    "            'nominal_size': alpha,\n",
    "            \n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # Store in results dictionary\n",
    "        results_by_order[current_order] = {\n",
    "            'empirical_sizes': order_results,\n",
    "            'best_tau': best_tau,\n",
    "            'best_tau_percentile': best_tau_percentile,\n",
    "            'true_tau_percentile': true_tau_percentile,\n",
    "            'estimated parameters': {\n",
    "                'mu': mu1,\n",
    "                'sigma': sigma1,\n",
    "                'alpha': alpha_hat\n",
    "            },\n",
    "            'true_parameters': {\n",
    "                'mu': mu_true,\n",
    "                'sigma': s2_true,\n",
    "                'alpha': alpha_true,\n",
    "                'tau': tau_true\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add to combined results\n",
    "        all_results.append({\n",
    "            'Order': current_order,\n",
    "            'R': R,  \n",
    "            'S': S,  \n",
    "            'Best_Tau': best_tau,\n",
    "            'Best_Tau_Percentile': best_tau_percentile,\n",
    "            'True_Tau': tau_true,\n",
    "            'True_Tau_Percentile': true_tau_percentile,\n",
    "            'mu_est': mu1,\n",
    "            'sigma_est': sigma1,\n",
    "            'alpha_est': alpha_hat,\n",
    "            'mu_true': mu_true,\n",
    "            'sigma_true': s2_true,\n",
    "            'alpha_true': alpha_true,\n",
    "            'Test 1': empirical_size_1,\n",
    "            'Test 1opt': empirical_size_1opt,\n",
    "            'Rejections_Test1': rejected_count_1,\n",
    "            'Rejections_Test1opt': rejected_count_1opt,\n",
    "            'M_Repetitions': M,\n",
    "            'Nominal_Size': alpha,\n",
    "            'Size_Distortion_Test1': abs(empirical_size_1 - alpha),\n",
    "            'Size_Distortion_Test1opt': abs(empirical_size_1opt - alpha)\n",
    "        })\n",
    "        \n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(f\"SUMMARY FOR ORDER {current_order}\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Selected tau: {best_tau} (true: {tau_true})\")\n",
    "            print(f\"Empirical Size Test 1: {empirical_size_1:.4f}\")\n",
    "            print(f\"Empirical Size Test 1opt: {empirical_size_1opt:.4f}\")\n",
    "            print(f\"Nominal Size: {alpha:.4f}\")\n",
    "            print(\"=\" * 80)\n",
    "    \n",
    "    # Create combined results DataFrame\n",
    "    combined_results = pd.DataFrame(all_results)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n\" + \"=\" * 100)\n",
    "        print(\"FINAL EMPIRICAL SIZE RESULTS\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        \n",
    "        # Create a more readable display table\n",
    "        display_columns = ['R', 'S', 'Order', 'Best_Tau', 'mu_est', 'sigma_est', 'alpha_est', \n",
    "                        'Nominal_Size', 'Test 1', 'Test 1opt']\n",
    "        \n",
    "        print(combined_results[display_columns].to_string(index=False, float_format='%.4f'))\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Summary assessment\n",
    "        print(f\"\\nEMPIRICAL SIZE ASSESSMENT:\")\n",
    "        print(\"-\" * 50)\n",
    "        for _, row in combined_results.iterrows():\n",
    "            order = int(row['Order'])\n",
    "            size2 = row['Test 1']\n",
    "            size4 = row['Test 1opt']\n",
    "            \n",
    "            \n",
    "            print(f\"Order {order}:\")\n",
    "            print(f\"  Test 1: {size2:.4f} \")\n",
    "            print(f\"  Test 1opt: {size4:.4f} \")\n",
    "    \n",
    "    # ADDED: Display beta selection details\n",
    "    display_beta_selection_details(beta_selection_tracker, c_values, N, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust parallelization functions loaded!\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "def application_analysisl_parallel(n_data=2000, mu_true=9.0, s2_true=1.17, alpha_true=3.05, \n",
    "                          tau_true=32000, tau_perc_true=0.82, M=10, S=200, R=299, B=299, \n",
    "                          alpha=0.05, orders=[4,8], c_values=None, tau_grid=None, \n",
    "                          seed=1234, verbose=True, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Parallelized version with robust error handling and fallback options.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine number of cores\n",
    "    if n_jobs == -1:\n",
    "        n_cores = mp.cpu_count()\n",
    "    elif n_jobs == 1:\n",
    "        n_cores = 1\n",
    "    else:\n",
    "        n_cores = min(n_jobs, mp.cpu_count())\n",
    "    \n",
    "    if verbose:\n",
    "        if n_cores > 1:\n",
    "            print(f\"Using parallelization with {n_cores} cores\")\n",
    "        else:\n",
    "            print(\"Running sequentially (no parallelization)\")\n",
    "    \n",
    "    # FIRST: Try to run original function to make sure it works\n",
    "    try:\n",
    "        if verbose:\n",
    "            print(\"Testing original function availability...\")\n",
    "        \n",
    "        # Test with very small parameters\n",
    "        test_result = application_analysisl(\n",
    "            n_data=100, M=1, S=5, R=5, orders=orders[:1], verbose=False\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"✓ Original function test successful\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"✗ Original function test failed: {e}\")\n",
    "            print(\"Falling back to sequential execution only\")\n",
    "        n_cores = 1\n",
    "    \n",
    "    # If sequential or test failed, use original function\n",
    "    if n_cores == 1:\n",
    "        return application_analysisl(\n",
    "            n_data=n_data, mu_true=mu_true, s2_true=s2_true, alpha_true=alpha_true,\n",
    "            tau_true=tau_true, tau_perc_true=tau_perc_true, M=M, S=S, R=R, B=B,\n",
    "            alpha=alpha, orders=orders, c_values=c_values, tau_grid=tau_grid,\n",
    "            seed=seed, verbose=verbose\n",
    "        )\n",
    "    \n",
    "    # Calculate chunk size - make chunks larger to reduce overhead\n",
    "    min_chunk_size = 10  # Minimum chunk size\n",
    "    chunk_size = max(min_chunk_size, M // n_cores)  # One chunk per core\n",
    "    n_chunks = (M + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Splitting {M} Monte Carlo repetitions into {n_chunks} chunks of ~{chunk_size} each\")\n",
    "    \n",
    "    # Prepare arguments for parallel execution\n",
    "    chunk_args = []\n",
    "    start_m = 0\n",
    "    \n",
    "    for chunk_id in range(n_chunks):\n",
    "        end_m = min(start_m + chunk_size, M)\n",
    "        actual_chunk_size = end_m - start_m\n",
    "        \n",
    "        chunk_seed = seed + chunk_id * 10000\n",
    "        \n",
    "        chunk_args.append({\n",
    "            'n_data': n_data,\n",
    "            'mu_true': mu_true, \n",
    "            's2_true': s2_true,\n",
    "            'alpha_true': alpha_true,\n",
    "            'tau_true': tau_true,\n",
    "            'tau_perc_true': tau_perc_true,\n",
    "            'M': actual_chunk_size,\n",
    "            'S': S,\n",
    "            'R': R, \n",
    "            'B': B,\n",
    "            'alpha': alpha,\n",
    "            'orders': orders,\n",
    "            'c_values': c_values,\n",
    "            'tau_grid': tau_grid,\n",
    "            'seed': chunk_seed,\n",
    "            'verbose': False,\n",
    "            'chunk_id': chunk_id\n",
    "        })\n",
    "        \n",
    "        start_m = end_m\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Starting parallel execution...\")\n",
    "    \n",
    "    # Try parallel execution with fallback to sequential\n",
    "    chunk_results = []\n",
    "    \n",
    "    try:\n",
    "        # OPTION 1: Try with ProcessPoolExecutor\n",
    "        with ProcessPoolExecutor(max_workers=min(n_cores, 4)) as executor:  # Limit to 4 cores max\n",
    "            future_to_chunk = {}\n",
    "            \n",
    "            # Submit jobs one by one with error checking\n",
    "            for chunk_arg in chunk_args:\n",
    "                try:\n",
    "                    future = executor.submit(run_chunk_analysis_safe, **chunk_arg)\n",
    "                    future_to_chunk[future] = chunk_arg['chunk_id']\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Failed to submit chunk {chunk_arg['chunk_id']}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not future_to_chunk:\n",
    "                raise Exception(\"No chunks could be submitted\")\n",
    "            \n",
    "            # Collect results\n",
    "            completed_chunks = 0\n",
    "            for future in as_completed(future_to_chunk):\n",
    "                chunk_id = future_to_chunk[future]\n",
    "                try:\n",
    "                    result = future.result(timeout=300)  # 5 minute timeout per chunk\n",
    "                    if result is not None:\n",
    "                        chunk_results.append(result)\n",
    "                    completed_chunks += 1\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"✓ Completed chunk {chunk_id + 1}/{len(chunk_args)} ({completed_chunks/len(chunk_args)*100:.1f}%)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"✗ Error in chunk {chunk_id}: {str(e)[:100]}\")\n",
    "                    continue\n",
    "                    \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Parallel execution failed: {e}\")\n",
    "            print(\"Falling back to sequential execution...\")\n",
    "        \n",
    "        # FALLBACK: Sequential execution of chunks\n",
    "        for i, chunk_arg in enumerate(chunk_args):\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(f\"Running chunk {i+1}/{len(chunk_args)} sequentially...\")\n",
    "                \n",
    "                result = run_chunk_analysis_safe(**chunk_arg)\n",
    "                if result is not None:\n",
    "                    chunk_results.append(result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Error in sequential chunk {i}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Check if we got any results\n",
    "    if not chunk_results:\n",
    "        if verbose:\n",
    "            print(\"No chunks completed successfully. Running original function...\")\n",
    "        \n",
    "        return application_analysisl(\n",
    "            n_data=n_data, mu_true=mu_true, s2_true=s2_true, alpha_true=alpha_true,\n",
    "            tau_true=tau_true, tau_perc_true=tau_perc_true, M=M, S=S, R=R, B=B,\n",
    "            alpha=alpha, orders=orders, c_values=c_values, tau_grid=tau_grid,\n",
    "            seed=seed, verbose=verbose\n",
    "        )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Combining results from {len(chunk_results)} successful chunks...\")\n",
    "    \n",
    "    # Combine results from successful chunks\n",
    "    return combine_chunk_results(chunk_results, orders, verbose)\n",
    "\n",
    "def run_chunk_analysis_safe(**kwargs):\n",
    "    \"\"\"\n",
    "    Safely run application_analysisl for a single chunk with comprehensive error handling.\n",
    "    \"\"\"\n",
    "    chunk_id = kwargs.pop('chunk_id', 0)\n",
    "    \n",
    "    try:\n",
    "        # Import required modules in worker process\n",
    "        import sys\n",
    "        import os\n",
    "        \n",
    "        # Try to run the original function\n",
    "        result = application_analysisl(**kwargs)\n",
    "        return result\n",
    "        \n",
    "    except NameError as e:\n",
    "        # Function not available in worker process\n",
    "        print(f\"Chunk {chunk_id}: Function not available: {e}\")\n",
    "        return None\n",
    "        \n",
    "    except ImportError as e:\n",
    "        # Import error in worker process\n",
    "        print(f\"Chunk {chunk_id}: Import error: {e}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Any other error\n",
    "        print(f\"Chunk {chunk_id}: Error: {str(e)[:200]}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Alternative approach using multiprocessing.Pool\n",
    "def application_analysisl_parallel_mp(n_data=2000, mu_true=9.0, s2_true=1.17, alpha_true=3.05, \n",
    "                          tau_true=32000, tau_perc_true=0.82, M=10, S=200, R=299, B=299, \n",
    "                          alpha=0.05, orders=[4,8], c_values=None, tau_grid=None, \n",
    "                          seed=1234, verbose=True, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Alternative implementation using multiprocessing.Pool instead of ProcessPoolExecutor\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_jobs == 1 or M < 20:  # Use sequential for small jobs\n",
    "        return application_analysisl(\n",
    "            n_data=n_data, mu_true=mu_true, s2_true=s2_true, alpha_true=alpha_true,\n",
    "            tau_true=tau_true, tau_perc_true=tau_perc_true, M=M, S=S, R=R, B=B,\n",
    "            alpha=alpha, orders=orders, c_values=c_values, tau_grid=tau_grid,\n",
    "            seed=seed, verbose=verbose\n",
    "        )\n",
    "    \n",
    "    # Use threading instead of multiprocessing for Jupyter compatibility\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    \n",
    "    n_cores = min(4, mp.cpu_count()) if n_jobs == -1 else min(n_jobs, 4)\n",
    "    chunk_size = max(5, M // n_cores)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Using ThreadPoolExecutor with {n_cores} threads\")\n",
    "        print(f\"Chunk size: {chunk_size}\")\n",
    "    \n",
    "    # Prepare chunks\n",
    "    chunk_args = []\n",
    "    for i in range(0, M, chunk_size):\n",
    "        actual_size = min(chunk_size, M - i)\n",
    "        chunk_args.append({\n",
    "            'n_data': n_data, 'mu_true': mu_true, 's2_true': s2_true,\n",
    "            'alpha_true': alpha_true, 'tau_true': tau_true, 'tau_perc_true': tau_perc_true,\n",
    "            'M': actual_size, 'S': S, 'R': R, 'B': B, 'alpha': alpha,\n",
    "            'orders': orders, 'c_values': c_values, 'tau_grid': tau_grid,\n",
    "            'seed': seed + i * 1000, 'verbose': False\n",
    "        })\n",
    "    \n",
    "    # Execute with ThreadPoolExecutor\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_cores) as executor:\n",
    "        futures = [executor.submit(application_analysisl, **args) for args in chunk_args]\n",
    "        \n",
    "        for i, future in enumerate(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                if verbose:\n",
    "                    print(f\"Completed thread {i+1}/{len(futures)}\")\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Thread {i+1} failed: {e}\")\n",
    "    \n",
    "    if not results:\n",
    "        # Fallback to sequential\n",
    "        return application_analysisl(\n",
    "            n_data=n_data, mu_true=mu_true, s2_true=s2_true, alpha_true=alpha_true,\n",
    "            tau_true=tau_true, tau_perc_true=tau_perc_true, M=M, S=S, R=R, B=B,\n",
    "            alpha=alpha, orders=orders, c_values=c_values, tau_grid=tau_grid,\n",
    "            seed=seed, verbose=verbose\n",
    "        )\n",
    "    \n",
    "    return combine_chunk_results(results, orders, verbose)\n",
    "\n",
    "# Keep the combine_chunk_results function as is...\n",
    "def combine_chunk_results(chunk_results, orders, verbose=True):\n",
    "    \"\"\"Combine results from multiple parallel chunks into a single result structure.\"\"\"\n",
    "    # [Your existing combine_chunk_results function - no changes needed]\n",
    "    \n",
    "    if not chunk_results:\n",
    "        raise ValueError(\"No chunk results to combine\")\n",
    "    \n",
    "    # Initialize combined results structure\n",
    "    combined_results_by_order = {}\n",
    "    combined_all_results = []\n",
    "    combined_beta_details = {}\n",
    "    \n",
    "    # Use data from first chunk (should be identical across chunks)\n",
    "    true_data = chunk_results[0]['true_data']\n",
    "    generation_parameters = chunk_results[0]['generation_parameters']\n",
    "    \n",
    "    # Process each order\n",
    "    for order in orders:\n",
    "        # Initialize counters for this order\n",
    "        total_rejections_test1 = 0\n",
    "        total_rejections_test1opt = 0\n",
    "        total_M = 0\n",
    "        \n",
    "        # Collect data from all chunks for this order\n",
    "        order_data = []\n",
    "        beta_selections = []\n",
    "        \n",
    "        # Representative values (should be same across chunks)\n",
    "        best_tau = None\n",
    "        estimated_params = None\n",
    "        true_params = None\n",
    "        \n",
    "        for chunk_result in chunk_results:\n",
    "            if order in chunk_result['results_by_order']:\n",
    "                chunk_order_data = chunk_result['results_by_order'][order]\n",
    "                \n",
    "                # Accumulate rejection counts\n",
    "                total_rejections_test1 += chunk_order_data['empirical_sizes']['rejections_test1']\n",
    "                total_rejections_test1opt += chunk_order_data['empirical_sizes']['rejections_test1opt']\n",
    "                total_M += chunk_order_data['empirical_sizes']['M_repetitions']\n",
    "                \n",
    "                # Store representative values (should be same across chunks)\n",
    "                if best_tau is None:\n",
    "                    best_tau = chunk_order_data['best_tau']\n",
    "                    estimated_params = chunk_order_data['estimated_parameters']\n",
    "                    true_params = chunk_order_data['true_parameters']\n",
    "                \n",
    "                # Collect individual results\n",
    "                for _, row in chunk_result['combined_results'].iterrows():\n",
    "                    if row['Order'] == order:\n",
    "                        order_data.append(row.to_dict())\n",
    "            \n",
    "            # Collect beta selection details\n",
    "            if order in chunk_result.get('beta_selection_details', {}):\n",
    "                beta_selections.extend(chunk_result['beta_selection_details'][order])\n",
    "        \n",
    "        if total_M > 0:\n",
    "            # Calculate combined empirical sizes\n",
    "            combined_empirical_size_1 = total_rejections_test1 / total_M\n",
    "            combined_empirical_size_1opt = total_rejections_test1opt / total_M\n",
    "            \n",
    "            # Store combined results for this order\n",
    "            combined_results_by_order[order] = {\n",
    "                'empirical_sizes': {\n",
    "                    'empirical_size_test1': combined_empirical_size_1,\n",
    "                    'empirical_size_test1opt': combined_empirical_size_1opt,\n",
    "                    'rejections_test1': total_rejections_test1,\n",
    "                    'rejections_test1opt': total_rejections_test1opt,\n",
    "                    'M_repetitions': total_M,\n",
    "                    'nominal_size': order_data[0]['Nominal_Size'] if order_data else 0.05,\n",
    "                },\n",
    "                'best_tau': best_tau,\n",
    "                'best_tau_percentile': order_data[0]['Best_Tau_Percentile'] if order_data else None,\n",
    "                'true_tau_percentile': order_data[0]['True_Tau_Percentile'] if order_data else None,\n",
    "                'estimated_parameters': estimated_params,\n",
    "                'true_parameters': true_params\n",
    "            }\n",
    "            \n",
    "            # Add combined result to all_results\n",
    "            if order_data:\n",
    "                combined_row = order_data[0].copy()  # Use first chunk as template\n",
    "                combined_row.update({\n",
    "                    'Test 1': combined_empirical_size_1,\n",
    "                    'Test 1opt': combined_empirical_size_1opt,\n",
    "                    'Rejections_Test1': total_rejections_test1,\n",
    "                    'Rejections_Test1opt': total_rejections_test1opt,\n",
    "                    'M_Repetitions': total_M,\n",
    "                    'Size_Distortion_Test1': abs(combined_empirical_size_1 - combined_row['Nominal_Size']),\n",
    "                    'Size_Distortion_Test1opt': abs(combined_empirical_size_1opt - combined_row['Nominal_Size'])\n",
    "                })\n",
    "                combined_all_results.append(combined_row)\n",
    "            \n",
    "            # Store beta selection details\n",
    "            if beta_selections:\n",
    "                combined_beta_details[order] = beta_selections\n",
    "    \n",
    "    # Create combined DataFrame\n",
    "    combined_results_df = pd.DataFrame(combined_all_results) if combined_all_results else pd.DataFrame()\n",
    "    \n",
    "    # Display final results\n",
    "    if verbose and not combined_results_df.empty:\n",
    "        print(f\"\\n\" + \"=\" * 100)\n",
    "        print(\"PARALLEL EXECUTION - FINAL COMBINED RESULTS\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        display_columns = ['R', 'S', 'Order', 'Best_Tau', 'mu_est', 'sigma_est', 'alpha_est', \n",
    "                          'Nominal_Size', 'Test 1', 'Test 1opt', 'M_Repetitions']\n",
    "        \n",
    "        available_columns = [col for col in display_columns if col in combined_results_df.columns]\n",
    "        print(combined_results_df[available_columns].to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Summary assessment\n",
    "        print(f\"\\nEMPIRICAL SIZE ASSESSMENT (COMBINED):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for _, row in combined_results_df.iterrows():\n",
    "            order = int(row['Order'])\n",
    "            size1 = row['Test 1']\n",
    "            size1opt = row['Test 1opt']\n",
    "            \n",
    "            print(f\"Order {order}: Test 1 = {size1:.4f}, Test 1opt = {size1opt:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'results_by_order': combined_results_by_order,\n",
    "        'combined_results': combined_results_df,\n",
    "        'true_data': true_data,\n",
    "        'beta_selection_details': combined_beta_details,\n",
    "        'generation_parameters': generation_parameters\n",
    "    }\n",
    "\n",
    "print(\"Robust parallelization functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parallelization with 7 cores\n",
      "Testing original function availability...\n",
      "✓ Original function test successful\n",
      "Splitting 10 Monte Carlo repetitions into 1 chunks of ~10 each\n",
      "Starting parallel execution...\n",
      "✗ Error in chunk 0: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "No chunks completed successfully. Running original function...\n",
      "================================================================================\n",
      "GENERATING DATA USING generate_hybrid_sample\n",
      "================================================================================\n",
      "Data generation parameters:\n",
      "  n = 2000\n",
      "  mu = 9.0\n",
      "  s2 (variance) = 1.17\n",
      "  alpha = 3.05\n",
      "  tau = 32000\n",
      "  tau_perc = 0.82\n",
      "  seed = 1234\n",
      "\n",
      "Generated data summary:\n",
      "  Total observations: 2000\n",
      "  Range: 121.77 to 174613.75\n",
      "  Mean: 16167.60\n",
      "  Below tau_true: 1641 (82.0%)\n",
      "  Above tau_true: 359 (17.9%)\n",
      "\n",
      "Analysis parameters:\n",
      "  Testing moment orders: [4, 8]\n",
      "  Monte Carlo repetitions (M): 10\n",
      "  Significance level (alpha): 0.05\n",
      "  Tau values range: 17388.60 to 38990.84\n",
      "\n",
      "================================================================================\n",
      "EMPIRICAL SIZE ANALYSIS - MAIN EXECUTION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROCESSING MOMENT ORDER: 4\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/process.py\", line 237, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'run_chunk_analysis_safe' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Evaluating optimal tau for order 4...\n",
      "\n",
      "TAU GRID AND VALUES TABLE:\n",
      "========================================\n",
      "Percentile   Tau Value      \n",
      "----------------------------------------\n",
      "    70.0th       17388.60\n",
      "    71.0th       18140.97\n",
      "    72.0th       18737.04\n",
      "    73.0th       19664.36\n",
      "    74.0th       20632.31\n",
      "    75.0th       21262.34\n",
      "    76.0th       22712.27\n",
      "    77.0th       23628.69\n",
      "    78.0th       24794.30\n",
      "    79.0th       25869.86\n",
      "    80.0th       27360.81\n",
      "    81.0th       29427.13\n",
      "    82.0th       32000.00\n",
      "    83.0th       32432.68\n",
      "    84.0th       33032.71\n",
      "    85.0th       33800.27\n",
      "    86.0th       34484.95\n",
      "    87.0th       35385.12\n",
      "    88.0th       36625.93\n",
      "    89.0th       37598.51\n",
      "    90.0th       38990.84\n",
      "========================================\n",
      "Selected tau: 33032.71 (84.0th percentile)\n",
      "True tau: 32000.00 (82.0th percentile)\n",
      "Data split: 1680 below tau, 320 above tau\n",
      "\n",
      "Testing different alpha estimation methods:\n",
      "------------------------------------------------------------\n",
      "mle_robust     : α = 3.9279, error = 0.8779 (28.79%)\n",
      "hill           : α = 2.9857, error = 0.0643 (2.11%)\n",
      "method_moments : α = 3.1627, error = 0.1127 (3.69%)\n",
      "combined       : α = 3.4922, error = 0.4422 (14.50%)\n",
      "\n",
      "Best method: hill\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS WITH IMPROVED ESTIMATION\n",
      "============================================================\n",
      "\n",
      "Parameter    True Value   Estimated    Error        Rel Error % \n",
      "------------------------------------------------------------\n",
      "mu           9.0000       9.0643       0.0643       0.71        \n",
      "sigma2       1.1700       1.1784       0.0084       0.72        \n",
      "alpha        3.0500       2.9857       0.0643       2.11        \n",
      "\n",
      "Estimated vs True parameters for order 4:\n",
      "  mu: 9.064311 (true: 9.000000)\n",
      "  sigma: 1.178384 (true: 1.170000)\n",
      "  alpha: 2.985658 (true: 3.050000)\n",
      "\n",
      "Step 2: Computing empirical sizes for order 4 with M=10 repetitions...\n",
      "  Progress: 1/10 (10.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "Order 4 - Selected Beta: 0.7937005259840998\n",
      "  Progress: 2/10 (20.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "  Progress: 3/10 (30.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "  Progress: 4/10 (40.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "  Progress: 5/10 (50.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "  Progress: 6/10 (60.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "  Progress: 7/10 (70.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "  Progress: 8/10 (80.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "  Progress: 9/10 (90.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "  Progress: 10/10 (100.0%)\n",
      "Selected theta: [np.float64(9.118475182635487), np.float64(1.2691605776033927), np.float64(2.157715092007838)]\n",
      "\n",
      "EMPIRICAL SIZE RESULTS FOR ORDER 4:\n",
      "  Test 1 (Unweighted): 0.1000 (1/10 rejections)\n",
      "  Test 1opt (Optimally Weighted): 0.1000 (1/10 rejections)\n",
      "  Expected (under null): 0.0500\n",
      "\n",
      "================================================================================\n",
      "SUMMARY FOR ORDER 4\n",
      "================================================================================\n",
      "Selected tau: 33032.71258264062 (true: 32000)\n",
      "Empirical Size Test 1: 0.1000\n",
      "Empirical Size Test 1opt: 0.1000\n",
      "Nominal Size: 0.0500\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROCESSING MOMENT ORDER: 8\n",
      "================================================================================\n",
      "\n",
      "Step 1: Evaluating optimal tau for order 8...\n",
      "\n",
      "TAU GRID AND VALUES TABLE:\n",
      "========================================\n",
      "Percentile   Tau Value      \n",
      "----------------------------------------\n",
      "    70.0th       17388.60\n",
      "    71.0th       18140.97\n",
      "    72.0th       18737.04\n",
      "    73.0th       19664.36\n",
      "    74.0th       20632.31\n",
      "    75.0th       21262.34\n",
      "    76.0th       22712.27\n",
      "    77.0th       23628.69\n",
      "    78.0th       24794.30\n",
      "    79.0th       25869.86\n",
      "    80.0th       27360.81\n",
      "    81.0th       29427.13\n",
      "    82.0th       32000.00\n",
      "    83.0th       32432.68\n",
      "    84.0th       33032.71\n",
      "    85.0th       33800.27\n",
      "    86.0th       34484.95\n",
      "    87.0th       35385.12\n",
      "    88.0th       36625.93\n",
      "    89.0th       37598.51\n",
      "    90.0th       38990.84\n",
      "========================================\n",
      "Selected tau: 35385.12 (87.0th percentile)\n",
      "True tau: 32000.00 (82.0th percentile)\n",
      "Data split: 1740 below tau, 260 above tau\n",
      "\n",
      "Testing different alpha estimation methods:\n",
      "------------------------------------------------------------\n",
      "mle_robust     : α = 3.5779, error = 0.5279 (17.31%)\n",
      "hill           : α = 2.9447, error = 0.1053 (3.45%)\n",
      "method_moments : α = 3.1496, error = 0.0996 (3.27%)\n",
      "combined       : α = 3.3023, error = 0.2523 (8.27%)\n",
      "\n",
      "Best method: method_moments\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS WITH IMPROVED ESTIMATION\n",
      "============================================================\n",
      "\n",
      "Parameter    True Value   Estimated    Error        Rel Error % \n",
      "------------------------------------------------------------\n",
      "mu           9.0000       9.1649       0.1649       1.83        \n",
      "sigma2       1.1700       1.3103       0.1403       11.99       \n",
      "alpha        3.0500       3.1496       0.0996       3.27        \n",
      "\n",
      "Estimated vs True parameters for order 8:\n",
      "  mu: 9.164914 (true: 9.000000)\n",
      "  sigma: 1.310281 (true: 1.170000)\n",
      "  alpha: 3.149628 (true: 3.050000)\n",
      "\n",
      "Step 2: Computing empirical sizes for order 8 with M=10 repetitions...\n",
      "  Progress: 1/10 (10.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/1hbxwjcj1g15h5z8zf8tykqc0000gn/T/ipykernel_75831/1905261377.py:679: RuntimeWarning: divide by zero encountered in matmul\n",
      "  A_pd = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
      "/var/folders/47/1hbxwjcj1g15h5z8zf8tykqc0000gn/T/ipykernel_75831/1905261377.py:679: RuntimeWarning: overflow encountered in matmul\n",
      "  A_pd = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
      "/var/folders/47/1hbxwjcj1g15h5z8zf8tykqc0000gn/T/ipykernel_75831/1905261377.py:679: RuntimeWarning: invalid value encountered in matmul\n",
      "  A_pd = eigvecs @ np.diag(eigvals) @ eigvecs.T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "Order 8 - Selected Beta: 3.968502629920499\n",
      "  Progress: 2/10 (20.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "  Progress: 3/10 (30.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "  Progress: 4/10 (40.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "  Progress: 5/10 (50.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "  Progress: 6/10 (60.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "  Progress: 7/10 (70.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "  Progress: 8/10 (80.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "  Progress: 9/10 (90.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "  Progress: 10/10 (100.0%)\n",
      "Selected theta: [np.float64(9.20484705065635), np.float64(1.409112616328262), np.float64(2.4159507727268017)]\n",
      "\n",
      "EMPIRICAL SIZE RESULTS FOR ORDER 8:\n",
      "  Test 1 (Unweighted): 0.2000 (2/10 rejections)\n",
      "  Test 1opt (Optimally Weighted): 0.0000 (0/10 rejections)\n",
      "  Expected (under null): 0.0500\n",
      "\n",
      "================================================================================\n",
      "SUMMARY FOR ORDER 8\n",
      "================================================================================\n",
      "Selected tau: 35385.121084888575 (true: 32000)\n",
      "Empirical Size Test 1: 0.2000\n",
      "Empirical Size Test 1opt: 0.0000\n",
      "Nominal Size: 0.0500\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "FINAL EMPIRICAL SIZE RESULTS\n",
      "====================================================================================================\n",
      "  R   S  Order   Best_Tau  mu_est  sigma_est  alpha_est  Nominal_Size  Test 1  Test 1opt\n",
      "299 200      4 33032.7126  9.0643     1.1784     2.9857        0.0500  0.1000     0.1000\n",
      "299 200      8 35385.1211  9.1649     1.3103     3.1496        0.0500  0.2000     0.0000\n",
      "====================================================================================================\n",
      "\n",
      "EMPIRICAL SIZE ASSESSMENT:\n",
      "--------------------------------------------------\n",
      "Order 4:\n",
      "  Test 1: 0.1000 \n",
      "  Test 1opt: 0.1000 \n",
      "Order 8:\n",
      "  Test 1: 0.2000 \n",
      "  Test 1opt: 0.0000 \n",
      "\n",
      "BETA SELECTION DETAILS - Moment Order: 4\n",
      "================================================================================\n",
      "c-value         Best Beta                 Count    Percentage  \n",
      "--------------------------------------------------------------------------------\n",
      "10              7.937005259840998e-01     10       100.00      %\n",
      "\n",
      "Total selections for order 4: 10\n",
      "\n",
      "BETA SELECTION DETAILS - Moment Order: 8\n",
      "================================================================================\n",
      "c-value         Best Beta                 Count    Percentage  \n",
      "--------------------------------------------------------------------------------\n",
      "50              3.968502629920499e+00     10       100.00      %\n",
      "\n",
      "Total selections for order 8: 10\n"
     ]
    }
   ],
   "source": [
    "# Try the safer version first\n",
    "#results_parallel = application_analysisl_parallel(M=10, orders=[4,8], n_jobs=2)\n",
    "\n",
    "# Or try the thread-based version for Jupyter compatibility\n",
    "#results_parallel = application_analysisl_parallel_mp(M=100, orders=[4,8], n_jobs=2)\n",
    "\n",
    "# For larger jobs after testing\n",
    "results_parallel = application_analysisl_parallel(M=1000, orders=[4,8,11,20], n_jobs=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
